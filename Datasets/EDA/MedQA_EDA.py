# -*- coding: utf-8 -*-
"""NLP_MedQA_EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Ay1CsIu5zZPW6OAf4EM7n8zt3F10s8M
"""

pip install datasets pandas

"""
Step 1: MedQA Dataset Acquisition and Loading
NLP Project - Contextual Fine-tuning for Medical QA
"""

from datasets import load_dataset
import pandas as pd

print("="*70)
print("STEP 1: MedQA DATASET ACQUISITION AND LOADING")
print("="*70)

# ============================================
# 1.1 Load the dataset from Hugging Face
# ============================================

print("\n[1/4] Loading MedQA dataset from Hugging Face...")
print("Source: GBaker/MedQA-USMLE-4-options")
print("This may take a minute on first run (downloads and caches locally)...\n")

try:
    # Load the dataset
    medqa_dataset = load_dataset("GBaker/MedQA-USMLE-4-options")
    print("‚úì Dataset loaded successfully!")

except Exception as e:
    print(f"‚úó Error loading dataset: {e}")
    print("\nTroubleshooting:")
    print("1. Check your internet connection")
    print("2. Try: pip install --upgrade datasets")
    print("3. Or use alternative: load_dataset('bigbio/med_qa', 'med_qa_en_bigbio_qa')")
    exit()

# ============================================
# 1.2 Inspect dataset structure
# ============================================

print("\n[2/4] Inspecting dataset structure...")
print("\nAvailable splits:")
for split_name in medqa_dataset.keys():
    print(f"  ‚Ä¢ {split_name}: {len(medqa_dataset[split_name])} examples")

print(f"\nTotal examples: {sum(len(medqa_dataset[split]) for split in medqa_dataset.keys())}")

# ============================================
# 1.3 Examine data schema
# ============================================

print("\n[3/4] Examining data schema...")
print("\nColumn names and types:")

# Get first example to see structure
first_example = medqa_dataset['train'][0]

for key, value in first_example.items():
    value_type = type(value).__name__
    print(f"  ‚Ä¢ {key}: {value_type}")

# ============================================
# 1.4 Display sample examples
# ============================================

print("\n[4/4] Displaying sample examples...")

print("\n" + "="*70)
print("SAMPLE EXAMPLE FROM TRAINING SET")
print("="*70)

example = medqa_dataset['train'][0]

print(f"\nüìã Question:")
print(f"{example['question']}\n")

print(f"üìù Options:")
for option_key, option_text in example['options'].items():
    # Mark the correct answer
    if option_key == example['answer_idx']:
        print(f"  ‚úì {option_key}) {option_text}")
    else:
        print(f"    {option_key}) {option_text}")

print(f"\n‚úÖ Correct Answer: {example['answer_idx']}")

# ============================================
# 1.5 Quick verification
# ============================================

print("\n" + "="*70)
print("VERIFICATION CHECKLIST")
print("="*70)

checks = []

# Check 1: All splits present
required_splits = ['train', 'test']
all_splits_present = all(split in medqa_dataset.keys() for split in required_splits)
checks.append(("All required splits present (train/val/test)", all_splits_present))

# Check 2: Non-empty splits
non_empty = all(len(medqa_dataset[split]) > 0 for split in required_splits)
checks.append(("All splits contain data", non_empty))

# Check 3: Required fields present
required_fields = ['question', 'options', 'answer_idx']
fields_present = all(field in first_example.keys() for field in required_fields)
checks.append(("Required fields present", fields_present))

# Check 4: Options have 4 choices
has_4_options = len(first_example['options']) == 4
checks.append(("4 answer options per question", has_4_options))

# Print results
for check_name, passed in checks:
    status = "‚úì" if passed else "‚úó"
    print(f"{status} {check_name}")

# ============================================
# 1.6 Summary
# ============================================

print("\n" + "="*70)
print("LOADING SUMMARY")
print("="*70)

all_passed = all(check[1] for check in checks)

if all_passed:
    print("\n‚úÖ SUCCESS! MedQA dataset loaded and verified.")
    print("\nDataset is ready for:")
    print("  ‚Üí Exploratory Data Analysis")
    print("  ‚Üí Quality Checks")
    print("  ‚Üí Baseline Evaluation")
else:
    print("\n‚ö†Ô∏è  WARNING: Some checks failed. Review the issues above.")

print("\n" + "="*70)

# ============================================
# 1.7 Save dataset info for later use
# ============================================

print("\n[OPTIONAL] Saving dataset statistics...")

dataset_info = {
    'source': 'GBaker/MedQA-USMLE-4-options',
    'splits': {split: len(medqa_dataset[split]) for split in medqa_dataset.keys()},
    'total_examples': sum(len(medqa_dataset[split]) for split in medqa_dataset.keys()),
    'fields': list(first_example.keys()),
    'sample_question_length': len(example['question'].split()),
}

print("\nDataset Information:")
for key, value in dataset_info.items():
    print(f"  {key}: {value}")

print("\n‚úì Step 1 Complete!")
print("\nNext: Run Exploratory Data Analysis (EDA)")

"""
Step 2: Exploratory Data Analysis (EDA)
NLP Project - Contextual Fine-tuning for Medical QA
"""

from datasets import load_dataset
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import numpy as np

# Set style for better visualizations
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (15, 10)

print("="*70)
print("STEP 2: EXPLORATORY DATA ANALYSIS (EDA)")
print("="*70)

# ============================================
# 2.1 Load the dataset
# ============================================

print("\n[1/6] Loading MedQA dataset...")
medqa_dataset = load_dataset("GBaker/MedQA-USMLE-4-options")
print("‚úì Dataset loaded\n")

# ============================================
# 2.2 Basic Statistics
# ============================================

print("="*70)
print("BASIC STATISTICS")
print("="*70)

for split_name in medqa_dataset.keys():
    split_data = medqa_dataset[split_name]
    print(f"\n{split_name.upper()} Split:")
    print(f"  ‚Ä¢ Total examples: {len(split_data)}")
    print(f"  ‚Ä¢ Percentage of total: {len(split_data)/11451*100:.1f}%")

# ============================================
# 2.3 Answer Distribution Analysis
# ============================================

print("\n" + "="*70)
print("ANSWER DISTRIBUTION ANALYSIS")
print("="*70)

def analyze_answer_distribution(dataset_split, split_name):
    """Analyze the distribution of correct answers"""

    answer_counts = Counter([ex['answer_idx'] for ex in dataset_split])
    total = len(dataset_split)

    print(f"\n{split_name} Split Answer Distribution:")
    print("-" * 40)

    for answer in ['A', 'B', 'C', 'D']:
        count = answer_counts.get(answer, 0)
        percentage = (count / total) * 100
        bar = "‚ñà" * int(percentage / 2)
        print(f"  {answer}: {count:4d} ({percentage:5.2f}%) {bar}")

    # Check if balanced (within 5% of 25%)
    balanced = all(20 <= (answer_counts.get(ans, 0)/total)*100 <= 30 for ans in ['A', 'B', 'C', 'D'])

    if balanced:
        print(f"  ‚úì Answer distribution is balanced")
    else:
        print(f"  ‚ö† Answer distribution may be skewed")

    return answer_counts

train_answers = analyze_answer_distribution(medqa_dataset['train'], "TRAIN")
test_answers = analyze_answer_distribution(medqa_dataset['test'], "TEST")

# ============================================
# 2.4 Question Length Analysis
# ============================================

print("\n" + "="*70)
print("QUESTION LENGTH ANALYSIS")
print("="*70)

def analyze_question_lengths(dataset_split, split_name):
    """Analyze question length statistics"""

    # Calculate lengths in words
    lengths_words = [len(ex['question'].split()) for ex in dataset_split]

    # Calculate lengths in characters
    lengths_chars = [len(ex['question']) for ex in dataset_split]

    print(f"\n{split_name} Split Question Lengths:")
    print("-" * 40)
    print(f"  Word Count Statistics:")
    print(f"    ‚Ä¢ Mean:   {np.mean(lengths_words):.1f} words")
    print(f"    ‚Ä¢ Median: {np.median(lengths_words):.1f} words")
    print(f"    ‚Ä¢ Min:    {min(lengths_words)} words")
    print(f"    ‚Ä¢ Max:    {max(lengths_words)} words")
    print(f"    ‚Ä¢ Std:    {np.std(lengths_words):.1f} words")

    print(f"\n  Character Count Statistics:")
    print(f"    ‚Ä¢ Mean:   {np.mean(lengths_chars):.1f} characters")
    print(f"    ‚Ä¢ Median: {np.median(lengths_chars):.1f} characters")

    # Categorize questions by length
    short = sum(1 for l in lengths_words if l < 50)
    medium = sum(1 for l in lengths_words if 50 <= l < 100)
    long = sum(1 for l in lengths_words if l >= 100)

    print(f"\n  Length Categories:")
    print(f"    ‚Ä¢ Short (<50 words):   {short} ({short/len(dataset_split)*100:.1f}%)")
    print(f"    ‚Ä¢ Medium (50-99 words): {medium} ({medium/len(dataset_split)*100:.1f}%)")
    print(f"    ‚Ä¢ Long (‚â•100 words):    {long} ({long/len(dataset_split)*100:.1f}%)")

    return lengths_words, lengths_chars

train_lengths, train_chars = analyze_question_lengths(medqa_dataset['train'], "TRAIN")
test_lengths, test_chars = analyze_question_lengths(medqa_dataset['test'], "TEST")

# ============================================
# 2.5 Data Quality Checks
# ============================================

print("\n" + "="*70)
print("DATA QUALITY CHECKS")
print("="*70)

def quality_checks(dataset_split, split_name):
    """Perform comprehensive quality checks"""

    print(f"\n{split_name} Split Quality Checks:")
    print("-" * 40)

    issues = []

    # Check 1: Missing data
    missing_questions = sum(1 for ex in dataset_split if not ex['question'] or ex['question'].strip() == '')
    missing_answers = sum(1 for ex in dataset_split if not ex['answer_idx'])

    print(f"  ‚Ä¢ Missing questions: {missing_questions}")
    print(f"  ‚Ä¢ Missing answers: {missing_answers}")

    if missing_questions > 0 or missing_answers > 0:
        issues.append("Missing data detected")

    # Check 2: Duplicate questions
    questions = [ex['question'] for ex in dataset_split]
    duplicates = len(questions) - len(set(questions))
    print(f"  ‚Ä¢ Duplicate questions: {duplicates}")

    if duplicates > 0:
        issues.append(f"{duplicates} duplicate questions")

    # Check 3: Option completeness
    incomplete_options = sum(1 for ex in dataset_split if len(ex['options']) != 4)
    print(f"  ‚Ä¢ Questions with ‚â†4 options: {incomplete_options}")

    if incomplete_options > 0:
        issues.append("Incomplete options detected")

    # Check 4: Valid answer indices
    invalid_answers = sum(1 for ex in dataset_split if ex['answer_idx'] not in ['A', 'B', 'C', 'D'])
    print(f"  ‚Ä¢ Invalid answer indices: {invalid_answers}")

    if invalid_answers > 0:
        issues.append("Invalid answer indices")

    # Summary
    if not issues:
        print(f"\n  ‚úÖ All quality checks passed!")
    else:
        print(f"\n  ‚ö†Ô∏è  Issues found:")
        for issue in issues:
            print(f"      - {issue}")

    return len(issues) == 0

train_quality = quality_checks(medqa_dataset['train'], "TRAIN")
test_quality = quality_checks(medqa_dataset['test'], "TEST")

# ============================================
# 2.6 Sample Questions Examination
# ============================================

print("\n" + "="*70)
print("SAMPLE QUESTIONS EXAMINATION")
print("="*70)

print("\nExamining 3 diverse examples from test set:\n")

# Get examples from different length categories
test_word_lengths = [(i, len(medqa_dataset['test'][i]['question'].split()))
                     for i in range(len(medqa_dataset['test']))]
test_word_lengths.sort(key=lambda x: x[1])

# Short, medium, long examples
short_idx = test_word_lengths[len(test_word_lengths)//4][0]
medium_idx = test_word_lengths[len(test_word_lengths)//2][0]
long_idx = test_word_lengths[3*len(test_word_lengths)//4][0]

sample_indices = [short_idx, medium_idx, long_idx]
categories = ["SHORT", "MEDIUM", "LONG"]

for idx, category in zip(sample_indices, categories):
    example = medqa_dataset['test'][idx]
    word_count = len(example['question'].split())

    print(f"\n{'='*70}")
    print(f"Example {category} Question ({word_count} words)")
    print(f"{'='*70}")
    print(f"\nüìã Question:\n{example['question']}\n")
    print(f"üìù Options:")
    for key, val in example['options'].items():
        marker = "‚úì‚úì‚úì" if key == example['answer_idx'] else "   "
        print(f"  {marker} {key}) {val}")
    print(f"\n‚úÖ Correct Answer: {example['answer_idx']}")

# ============================================
# 2.7 Create Visualizations
# ============================================

print("\n" + "="*70)
print("CREATING VISUALIZATIONS")
print("="*70)

fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('MedQA Dataset - Exploratory Data Analysis', fontsize=16, fontweight='bold')

# Plot 1: Answer Distribution Comparison
ax1 = axes[0, 0]
x = np.arange(4)
width = 0.35
answers = ['A', 'B', 'C', 'D']
train_counts = [train_answers.get(ans, 0) for ans in answers]
test_counts = [test_answers.get(ans, 0) for ans in answers]

ax1.bar(x - width/2, train_counts, width, label='Train', alpha=0.8, color='#3498db')
ax1.bar(x + width/2, test_counts, width, label='Test', alpha=0.8, color='#e74c3c')
ax1.set_xlabel('Answer Option', fontsize=12, fontweight='bold')
ax1.set_ylabel('Count', fontsize=12, fontweight='bold')
ax1.set_title('Answer Distribution Across Splits', fontsize=14, fontweight='bold')
ax1.set_xticks(x)
ax1.set_xticklabels(answers)
ax1.legend()
ax1.grid(axis='y', alpha=0.3)

# Add percentage labels
for i, (train_c, test_c) in enumerate(zip(train_counts, test_counts)):
    train_pct = (train_c / sum(train_counts)) * 100
    test_pct = (test_c / sum(test_counts)) * 100
    ax1.text(i - width/2, train_c + 50, f'{train_pct:.1f}%', ha='center', va='bottom', fontsize=9)
    ax1.text(i + width/2, test_c + 5, f'{test_pct:.1f}%', ha='center', va='bottom', fontsize=9)

# Plot 2: Question Length Distribution
ax2 = axes[0, 1]
ax2.hist(train_lengths, bins=50, alpha=0.6, label='Train', color='#3498db', edgecolor='black')
ax2.hist(test_lengths, bins=50, alpha=0.6, label='Test', color='#e74c3c', edgecolor='black')
ax2.set_xlabel('Question Length (words)', fontsize=12, fontweight='bold')
ax2.set_ylabel('Frequency', fontsize=12, fontweight='bold')
ax2.set_title('Question Length Distribution', fontsize=14, fontweight='bold')
ax2.legend()
ax2.axvline(np.mean(train_lengths), color='#3498db', linestyle='--', linewidth=2, label=f'Train Mean: {np.mean(train_lengths):.1f}')
ax2.axvline(np.mean(test_lengths), color='#e74c3c', linestyle='--', linewidth=2, label=f'Test Mean: {np.mean(test_lengths):.1f}')
ax2.grid(alpha=0.3)

# Plot 3: Dataset Split Sizes
ax3 = axes[1, 0]
splits = ['Train', 'Test']
sizes = [len(medqa_dataset['train']), len(medqa_dataset['test'])]
colors = ['#3498db', '#e74c3c']
bars = ax3.bar(splits, sizes, color=colors, alpha=0.8, edgecolor='black', linewidth=2)
ax3.set_ylabel('Number of Examples', fontsize=12, fontweight='bold')
ax3.set_title('Dataset Split Sizes', fontsize=14, fontweight='bold')
ax3.grid(axis='y', alpha=0.3)

# Add value labels and percentages
for i, (bar, size) in enumerate(zip(bars, sizes)):
    height = bar.get_height()
    percentage = (size / sum(sizes)) * 100
    ax3.text(bar.get_x() + bar.get_width()/2., height,
            f'{size:,}\n({percentage:.1f}%)',
            ha='center', va='bottom', fontsize=12, fontweight='bold')

# Plot 4: Box Plot - Question Length Comparison
ax4 = axes[1, 1]
box_data = [train_lengths, test_lengths]
bp = ax4.boxplot(box_data, labels=['Train', 'Test'], patch_artist=True,
                 boxprops=dict(facecolor='lightblue', alpha=0.7),
                 medianprops=dict(color='red', linewidth=2),
                 whiskerprops=dict(linewidth=1.5),
                 capprops=dict(linewidth=1.5))

# Color the boxes
colors_box = ['#3498db', '#e74c3c']
for patch, color in zip(bp['boxes'], colors_box):
    patch.set_facecolor(color)
    patch.set_alpha(0.6)

ax4.set_ylabel('Question Length (words)', fontsize=12, fontweight='bold')
ax4.set_title('Question Length Distribution by Split', fontsize=14, fontweight='bold')
ax4.grid(axis='y', alpha=0.3)

# Add statistics
for i, data in enumerate(box_data, 1):
    median = np.median(data)
    q1 = np.percentile(data, 25)
    q3 = np.percentile(data, 75)
    ax4.text(i, median, f'Median: {median:.0f}', ha='left', va='center', fontsize=9)

plt.tight_layout()
plt.savefig('medqa_eda_analysis.png', dpi=300, bbox_inches='tight')
print("\n‚úì Visualizations saved as 'medqa_eda_analysis.png'")
print("  (Check your working directory for the image)")

# ============================================
# 2.8 Summary Report
# ============================================

print("\n" + "="*70)
print("EDA SUMMARY REPORT")
print("="*70)

print(f"""
‚úÖ Dataset Overview:
   ‚Ä¢ Total Examples: 11,451
   ‚Ä¢ Train: 10,178 (88.9%)
   ‚Ä¢ Test: 1,273 (11.1%)

‚úÖ Answer Distribution:
   ‚Ä¢ Train: Balanced across A/B/C/D (~25% each)
   ‚Ä¢ Test: Balanced across A/B/C/D (~25% each)

‚úÖ Question Characteristics:
   ‚Ä¢ Average Length: ~{np.mean(train_lengths):.0f} words
   ‚Ä¢ Range: {min(train_lengths)}-{max(train_lengths)} words
   ‚Ä¢ Majority are clinical case scenarios

‚úÖ Data Quality:
   ‚Ä¢ No missing values detected
   ‚Ä¢ {len(set([ex['question'] for ex in medqa_dataset['train']]))} unique questions in train
   ‚Ä¢ All questions have 4 options (A, B, C, D)
   ‚Ä¢ All answer indices are valid

‚úÖ Key Insights:
   ‚Ä¢ Questions vary from short factual to long clinical cases
   ‚Ä¢ Dataset is well-balanced for unbiased evaluation
   ‚Ä¢ High quality, suitable for medical QA tasks
   ‚Ä¢ Good split ratio for training and testing
""")

print("="*70)
print("‚úì EDA COMPLETE!")
print("="*70)
print("\nNext Steps:")
print("  1. ‚úÖ Dataset loaded and verified")
print("  2. ‚úÖ EDA completed")
print("  3. ‚è≥ Ready for baseline evaluation")
print("="*70)
